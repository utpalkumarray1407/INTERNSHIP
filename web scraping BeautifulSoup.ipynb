{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1206e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9095186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60db6b2",
   "metadata": {},
   "source": [
    "QUESTION (1) :\n",
    "               Write a python program to display all the header tags from wikipedia.org and make data frame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50daa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Header\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "header_tags = soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "header_texts = [tag.get_text() for tag in header_tags]\n",
    "\n",
    "df = pd.DataFrame(header_texts, columns= [\"Header\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fe10b",
   "metadata": {},
   "source": [
    " QUESTION (2) : Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://presidentofindia.nic.in/former-presidents.htm and make data frame. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75aa1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c93e70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the table containing the information\n",
    "table = soup.find(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aecf05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the data\n",
    "names = []\n",
    "terms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "225216b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "for row in table.find_all(\"tr\"):\n",
    "    columns = row.find_all(\"td\")\n",
    "    name = columns[0].text.strip()\n",
    "    term = columns[1].text.strip()\n",
    "  # Append the data to the respective lists\n",
    "    names.append(name)\n",
    "    terms.append(term)\n",
    "    \n",
    "data = {\"Name\": names, \"Term of Office\": terms}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1f1f0",
   "metadata": {},
   "source": [
    "QUESTION (3): Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "\n",
    "(a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718b6996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Teams Matches Points Rating\n",
      "0        India\\nIND      55  6,640    121\n",
      "1    Australia\\nAUS      42  4,926    117\n",
      "2  South Africa\\nSA      34  3,750    110\n",
      "3     Pakistan\\nPAK      36  3,922    109\n",
      "4   New Zealand\\nNZ      43  4,399    102\n",
      "5      England\\nENG      38  3,777     99\n",
      "6     Sri Lanka\\nSL      47  4,134     88\n",
      "7   Bangladesh\\nBAN      44  3,836     87\n",
      "8  Afghanistan\\nAFG      30  2,533     84\n",
      "9   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "teams = []\n",
    "\n",
    "for team in soup.select(\"tbody tr\"):\n",
    "    name = team.select(\"td\")[1].text.strip()\n",
    "    matches = team.select(\"td\")[2].text.strip()\n",
    "    points = team.select(\"td\")[3].text.strip()\n",
    "    rating = team.select(\"td\")[4].text.strip()\n",
    "    teams.append((name, matches, points, rating))\n",
    "    \n",
    "df = pd.DataFrame(teams, columns=[\"Teams\", \"Matches\",\"Points\",\"Rating\"])\n",
    "df = df.head(10)   \n",
    "print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393b3fd",
   "metadata": {},
   "source": [
    "(B) Top 10 ODI Batsmen along with the records of their team andrating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1da9fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsman Team Rating\n",
      "0           Shubman Gill  IND    826\n",
      "1             Babar Azam  PAK    824\n",
      "2            Virat Kohli  IND    791\n",
      "3           Rohit Sharma  IND    769\n",
      "4        Quinton de Kock   SA    760\n",
      "5         Daryl Mitchell   NZ    750\n",
      "6           David Warner  AUS    745\n",
      "7  Rassie van der Dussen   SA    735\n",
      "8           Harry Tector  IRE    729\n",
      "9            Dawid Malan  ENG    729\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    batsman = cells[1].text.strip()\n",
    "    team = cells[2].text.strip()\n",
    "    rating = cells[3].text.strip()\n",
    "    batsman_data.append([batsman, team, rating])\n",
    "    \n",
    "df = pd.DataFrame(batsman_data, columns= [ \"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ff5aa",
   "metadata": {},
   "source": [
    "(C) Top 10 ODI bowlers along with the records of their team andrating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4f155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Bowler Team Rating\n",
      "0  Keshav Maharaj   SA    741\n",
      "1  Josh Hazlewood  AUS    703\n",
      "2  Mohammed Siraj  IND    699\n",
      "3  Jasprit Bumrah  IND    685\n",
      "4      Adam Zampa  AUS    675\n",
      "5     Rashid Khan  AFG    667\n",
      "6   Kuldeep Yadav  IND    667\n",
      "7     Trent Boult   NZ    663\n",
      "8  Shaheen Afridi  PAK    650\n",
      "9  Mohammad Shami  IND    648\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    bowler = cells[1].text.strip()\n",
    "    team = cells[2].text.strip()\n",
    "    rating = cells[3].text.strip()\n",
    "    bowler_data.append([bowler, team, rating])\n",
    "    \n",
    "df = pd.DataFrame(bowler_data, columns= [ \"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b80ec",
   "metadata": {},
   "source": [
    "QUESTION (4): Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame- \n",
    "\n",
    "(a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8c99e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Teams Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,429    163\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      20  1,768     88\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      14  1,074     77\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      24  1,602     67\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "teams = []\n",
    "\n",
    "for team in soup.select(\"tbody tr\"):\n",
    "    name = team.select(\"td\")[1].text.strip()\n",
    "    matches = team.select(\"td\")[2].text.strip()\n",
    "    points = team.select(\"td\")[3].text.strip()\n",
    "    rating = team.select(\"td\")[4].text.strip()\n",
    "    teams.append((name, matches, points, rating))\n",
    "    \n",
    "df = pd.DataFrame(teams, columns=[\"Teams\", \"Matches\",\"Points\",\"Rating\"])\n",
    "df = df.head(10)   \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a7f05",
   "metadata": {},
   "source": [
    "(B) Top 10 women’s ODI Batting players along with the records of their team and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3aa4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Batsman Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    807\n",
      "1           Beth Mooney  AUS    750\n",
      "2   Chamari Athapaththu   SL    736\n",
      "3       Laura Wolvaardt   SA    727\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    698\n",
      "6          Ellyse Perry  AUS    697\n",
      "7      Harmanpreet Kaur  IND    694\n",
      "8           Meg Lanning  AUS    662\n",
      "9        Marizanne Kapp   SA    642\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    batsman = cells[1].text.strip()\n",
    "    team = cells[2].text.strip()\n",
    "    rating = cells[3].text.strip()\n",
    "    batsman_data.append([batsman, team, rating])\n",
    "    \n",
    "df = pd.DataFrame(batsman_data, columns= [ \"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab44b14",
   "metadata": {},
   "source": [
    "QUESTION (5): Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "\n",
    "(i) Headline \n",
    "(ii) Time \n",
    "(iii)News Link \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2b924d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline  \\\n",
      "0   S&P 500 rises on Friday to close at 2023 high:...   \n",
      "1   Fed Chair Powell calls talk of cutting rates '...   \n",
      "2   Treasury yields decline even as Powell says ra...   \n",
      "3   Bill Gates warns the world is likely to smash ...   \n",
      "4   UAE commits $30 billion to new climate-focused...   \n",
      "5   Progress on sustainability in the fashion indu...   \n",
      "6   Estonian PM calls on global powers to emulate ...   \n",
      "7   Bill Gates shares his ‘big hope’ for the COP28...   \n",
      "8   Op-ed: With continued geopolitical conflicts, ...   \n",
      "9   Zelenskyy says 'new phase of war' has begun, H...   \n",
      "10  Multiple civilians injured in strikes on Ukrai...   \n",
      "11  Russia slams Finland's border closure, warns t...   \n",
      "12  Finland to close entire border with Russia; wi...   \n",
      "13               CNBC's Sustainable Future Forum 2023   \n",
      "14  Wind power industry in moment of reckoning as ...   \n",
      "15  CEO on why natural gas infrastructure must be ...   \n",
      "16  Why one Tesla manager thinks used cars are 'ab...   \n",
      "17  Volvo Cars CEO strikes cautious tone on solid-...   \n",
      "18  Southeast Asia is on the cusp of a 'supercharg...   \n",
      "19  Cambodia's deputy prime minister: BRI has help...   \n",
      "20  Southeast Asia's first luxury hotel made from ...   \n",
      "21  Ahead of Indonesia’s elections, critics slam J...   \n",
      "22  Laos is spiraling toward a debt crisis as Chin...   \n",
      "23  Pack your jerseys. The era of 'sports tourism'...   \n",
      "24  See the photos that made National Geographic's...   \n",
      "25  The ultimate work perk? This company provides ...   \n",
      "26  Fear is driving Chinese travelers away from tw...   \n",
      "27  A UNESCO World Heritage site with thousands of...   \n",
      "28  Kevin Hart shares his No. 1 'secret weapon' fo...   \n",
      "29  Charlie Munger lived in the same home for 70 y...   \n",
      "30  Sandra Day O'Connor dies at 93—her legacy on t...   \n",
      "31  Harvard career expert: The No. 1 'desirable' t...   \n",
      "32  Your network is the key to your career, says T...   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/11/30/stock-market-t...  \n",
      "1   https://www.cnbc.com/2023/12/01/fed-chair-powe...  \n",
      "2   https://www.cnbc.com/2023/12/01/us-treasury-yi...  \n",
      "3   https://www.cnbc.com/2023/12/01/bill-gates-war...  \n",
      "4   https://www.cnbc.com/2023/12/01/uae-commits-30...  \n",
      "5   https://www.cnbc.com/video/2023/12/01/sustaina...  \n",
      "6   https://www.cnbc.com/video/2023/12/01/estonian...  \n",
      "7   https://www.cnbc.com/2023/12/01/bill-gates-sha...  \n",
      "8   https://www.cnbc.com/2023/12/01/op-ed-war-is-a...  \n",
      "9   https://www.cnbc.com/2023/12/01/russia-ukraine...  \n",
      "10  https://www.cnbc.com/2023/11/30/ukraine-war-li...  \n",
      "11  https://www.cnbc.com/2023/11/29/ukraine-war-li...  \n",
      "12  https://www.cnbc.com/2023/11/28/ukraine-war-li...  \n",
      "13  https://www.cnbc.com/2023/11/29/cnbcs-sustaina...  \n",
      "14  https://www.cnbc.com/2023/11/13/wind-power-ind...  \n",
      "15  https://www.cnbc.com/2023/11/09/ceo-on-why-nat...  \n",
      "16  https://www.cnbc.com/2023/11/02/one-tesla-mana...  \n",
      "17  https://www.cnbc.com/2023/10/27/volvo-cars-ceo...  \n",
      "18  https://www.cnbc.com/2023/11/29/southeast-asia...  \n",
      "19  https://www.cnbc.com/video/2023/11/27/cambodia...  \n",
      "20  https://www.cnbc.com/2023/11/27/a-new-luxury-h...  \n",
      "21  https://www.cnbc.com/2023/11/20/indonesia-pres...  \n",
      "22  https://www.cnbc.com/2023/11/09/laos-is-spiral...  \n",
      "23  https://www.cnbc.com/2023/11/27/pack-your-jers...  \n",
      "24  https://www.cnbc.com/2023/11/23/best-photograp...  \n",
      "25  https://www.cnbc.com/2023/11/23/remote-company...  \n",
      "26  https://www.cnbc.com/2023/11/20/fear-driving-c...  \n",
      "27  https://www.cnbc.com/2023/11/12/a-unesco-world...  \n",
      "28  https://www.cnbc.com/2023/12/01/kevin-hart-say...  \n",
      "29  https://www.cnbc.com/2023/12/01/charlie-munger...  \n",
      "30  https://www.cnbc.com/2023/12/01/sandra-day-oco...  \n",
      "31  https://www.cnbc.com/2023/12/01/the-no-1-desir...  \n",
      "32  https://www.cnbc.com/video/2023/12/01/your-net...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all(\"div\", class_=\"Card-titleContainer\")\n",
    "headlines = []\n",
    "links = []\n",
    "\n",
    "for article in articles:\n",
    "    headline = article.find(\"a\").text.strip()\n",
    "    headlines.append(headline)\n",
    "    link = article.find(\"a\")[\"href\"]\n",
    "    links.append(link)\n",
    "    \n",
    "data = {\"Headline\": headlines, \"News Link\": links}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac01f7",
   "metadata": {},
   "source": [
    "QUESTION (6): Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame- \n",
    "\n",
    "(i) Paper Title \n",
    "(ii) Authors \n",
    "(iii) Published Date \n",
    "(iv) Paper URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1045b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76e012b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_container = soup.find(\"div\", class_=\"pod-listing\")\n",
    "\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a0f82f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m \u001b[43marticles_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     title \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      3\u001b[0m     titles\u001b[38;5;241m.\u001b[39mappend(title)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "for article in articles_container.find_all(\"li\"):\n",
    "    title = article.find(\"h3\").text.strip()\n",
    "    titles.append(title)\n",
    "    author = article.find(\"span\", class_=\"text-xs\").text.strip()\n",
    "    authors.append(author)\n",
    "    date = article.find(\"span\", class_=\"text-xs\").find_next_sibling(\"span\").text.strip()\n",
    "    dates.append(date)\n",
    "    url = article.find(\"a\")[\"href\"]\n",
    "    urls.append(url)\n",
    "    \n",
    "    \n",
    "data = {\"Paper Title\": titles, \"Authors\": authors, \"Published Date\": dates, \"Paper URL\": urls }\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b766f",
   "metadata": {},
   "source": [
    "QUESTION (7): Write a python program to scrape mentioned details from dineout.co.in and make data frame- \n",
    "\n",
    "(i) Restaurant name \n",
    "(ii) Cuisine \n",
    "(iii) Location \n",
    "(iv) Ratings \n",
    "(v) Image URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11970889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df292f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_names = soup.find_all('h2', class_='restnt-name ellipsis')\n",
    "cuisines = soup.find_all('span', class_='double-line-ellipsis')\n",
    "locations = soup.find_all('span', class_='double-line-ellipsis')\n",
    "ratings = soup.find_all('span', class_='rating-value')\n",
    "image_urls = soup.find_all('img', class_='img-responsive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "923312ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list\n",
    "restaurant_list = []\n",
    "cuisine_list = []\n",
    "location_list = []\n",
    "rating_list = []\n",
    "image_url_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ffbd110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in restaurant_names:\n",
    "    restaurant_list.append(name.text.strip())\n",
    "    for cuisine in cuisines:\n",
    "        cuisine_list.append(cuisine.text.strip())\n",
    "        for location in locations:\n",
    "            location_list.append(location.text.strip())\n",
    "            for rating in ratings:\n",
    "                rating_list.append(rating.text.strip())\n",
    "                for image in image_urls:\n",
    "                    image_url_list.append(image['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99b67cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Restaurant Name': restaurant_list, 'Cuisine': cuisine_list, 'Location': location_list, 'Ratings': rating_list, 'Image URL': image_url_list }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d5780a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "898b80d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199e802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
